{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices\n",
    "\n",
    "This project involved building an interactive web app, using Shiny for Python, to display house prices in England and Wales, adding a Choropleth layer to visualise the differences by region. The aim was to demonstrate the Shiny software and investigate trends in house prices over time and across different regions of England and Wales. This notebook contains is analysis into the data and a walkthrough of how the app was built, but if you'd just like to view the app then use this link:\n",
    "\n",
    "**link**\n",
    "\n",
    "##### About the data\n",
    "\n",
    "For this project I used the Price Paid Data published monthly by the UK government, which tracks property sales in England and Wales submitted to HM Land Registry for registration. The dataset contains single residential properties sold for value since 1995, and since 2013 includes transfers under power of sale/repossessions, buy-to-lets (where identifiable by Mortgage type) and transfers to non-private individuals. This data is also used by property websites such as Zoopla and Rightmove to allow users to track property prices.\n",
    "\n",
    "The Price Paid Data includes information on all residential property sales in England and Wales that are sold for value and are lodged with us for registration. It excludes:\n",
    "\n",
    "- sales that have not been lodged with HM Land Registry\n",
    "- sales that were not for value\n",
    "- transfers, conveyances, assignments or leases at a premium with nominal rent, which are:\n",
    "- ‘Right to buy’ sales at a discount\n",
    "- subject to an existing mortgage\n",
    "- to effect the sale of a share in a property, for example, a transfer between parties on divorce\n",
    "- by way of a gift\n",
    "- under a compulsory purchase order\n",
    "- under a court order\n",
    "- to Trustees appointed under Deed of appointment\n",
    "- Vesting Deeds Transmissions or Assents of more than one property\n",
    "\n",
    "##### Initial planning\n",
    "\n",
    "- What granularity to use? e.g. county, town, postcode\n",
    "    - The dataset contains the postcode of each property, and every postcode has a clearly defined area, which should be available online\n",
    "    - The whole postcode is likely too granular - using the area code or district code should be a good compromise\n",
    "- How to aggregate price paid by area?\n",
    "    - Min, Max, Median, Mean\n",
    "    - Allow user to choose which statistic to show on the map\n",
    "    - Could also compare these summary statistics between points in time, which would highlight the areas which have seen the greatest changes in price over time\n",
    "\n",
    "##### Preparing the data\n",
    "\n",
    "Loading the CSV file into a MySQL database:\n",
    "\n",
    "~~~~sql\n",
    "DROP DATABASE IF EXISTS `houseprices`;\n",
    "CREATE DATABASE `houseprices`;\n",
    "USE `houseprices`;\n",
    "\n",
    "CREATE TABLE `pricePaid` (\n",
    "`unique_id` VARCHAR(100),\n",
    "`price_paid` DECIMAL,\n",
    "`deed_date` DATE,\n",
    "`postcode` VARCHAR(8),\n",
    "`property_type` VARCHAR(1),\n",
    "`new_build` VARCHAR(1),\n",
    "`estate_type` VARCHAR(1),\n",
    "`saon` VARCHAR(50),\n",
    "`paon` VARCHAR(50),\n",
    "`street` VARCHAR(50),\n",
    "`locality` VARCHAR(50),\n",
    "`town` VARCHAR(50),\n",
    "`district` VARCHAR(50),\n",
    "`county` VARCHAR(50),\n",
    "`transaction_category` VARCHAR(1),\n",
    "`linked_data_uri` VARCHAR(1),\n",
    "PRIMARY KEY (unique_id)\n",
    ");\n",
    "\n",
    "SET GLOBAL local_infile=ON;\n",
    "SET autocommit=0;\n",
    "SET unique_checks=1;\n",
    "SET foreign_key_checks=0;\n",
    "\n",
    "LOAD DATA LOW_PRIORITY \n",
    "LOCAL INFILE 'Path/To/Project/pricepaid.csv'\n",
    "INTO TABLE pricePaid \n",
    "CHARACTER SET armscii8\n",
    "FIELDS TERMINATED BY ','\n",
    "ENCLOSED BY '\"'\n",
    "LINES TERMINATED BY '\\n' \n",
    "(`unique_id`,`price_paid`,`deed_date`,`postcode`,`property_type`,`new_build`,`estate_type`,`saon`,`paon`,`street`,`locality`,`town`,`district`,`county`,`transaction_category`,`linked_data_uri`);\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had originally planned to use the full dataset in my Shiny app, however the full table is ~5GB in size with ~29m rows. For EDA purposes I am taking a sample of the data so that I can easily load the data into memory in Python. Taking a simple random sample of the data would mean that the number of samples from each area would be proportional to the population of that area, so to ensure that each area had an equal number of samples I would use stratified sampling instead.\n",
    "\n",
    "I needed to choose a level of granularity to which to stratify the data. A UK postcode is made up of 2 parts, the outward code (first part) and inward code (second part), separated by a space. The outward code consists of the postcode area (either 1 or 2 letters) followed by the postcode district (usually 1 or 2 digits). For example, in the postcode PO16 7GZ, PO16 is the outward code (or outcode), PO is the area and 16 is the district.\n",
    "\n",
    "OutCode was added as a generated column to the pricePaid table, along with a Year column and a YearBin column.\n",
    "\n",
    "~~~~sql\n",
    "ALTER TABLE pricePaid ADD COLUMN OutCode VARCHAR(4) GENERATED ALWAYS AS substr(postcode, 1, locate(' ', postcode) - 1) STORED;\n",
    "ALTER TABLE pricePaid ADD COLUMN Year INT GENERATED ALWAYS AS year(cast(deed_date as date)) STORED;\n",
    "ALTER TABLE pricePaid ADD COLUMN YearBin VARCHAR(4) GENERATED ALWAYS AS case when (Year < 2005) then '1995 - 2004' when (Year < 2015) then '2005 - 2014' else '2015 +' end STORED;\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an index on Outcode and YearBin to speed up the stratified sample query.\n",
    "\n",
    "~~~~sql\n",
    "CREATE INDEX OutcodeYearBinIndex ON pricepaid (Outcode, YearBin);\n",
    "~~~~\n",
    "\n",
    "Taking a stratified sample of 100 observations for each distinct OutCode and YearBin. This is to be used for EDA.\n",
    "\n",
    "~~~~sql\n",
    "SELECT t.* FROM\n",
    "(SELECT pp.*, ROW_NUMBER() OVER (PARTITION BY OutCode, YearBin ORDER BY RAND()) AS SeqNum\n",
    "FROM pricePaid pp) t\n",
    "WHERE t.SeqNum <= 100\n",
    "INTO LOCAL OUTFILE '/Path/To/Project/pricepaidsample.csv'\n",
    "FIELDS TERMINATED BY ','\n",
    "ENCLOSED BY '\"'\n",
    "LINES TERMINATED BY '\\n';\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EDA\n",
    "\n",
    "Importing the sample into Python and creating a density plot of all house prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "### Load data\n",
    "appDir = Path(os.path.abspath(''))\n",
    "dataset = pd.read_csv(appDir / \"pricepaidsample.csv\", delimiter='\\t', header=0, encoding=\"utf-8\")\n",
    "# remove missing postcodes and price paid values of zero\n",
    "dataset = dataset[~((dataset['postcode'].isnull()) | (dataset['price_paid'] == 0))]\n",
    "dataset['price_paid_mil'] = dataset['price_paid'] / 1000000\n",
    "\n",
    "# density plot of price paid\n",
    "fig, ax = plt.subplots()\n",
    "sns.kdeplot(df['price_paid_mil'])\n",
    "ax.set_xlabel('Price Paid (£m)')\n",
    "ax.set_xlim(left=-5)\n",
    "ax.set_ylim(bottom=-0.00005)\n",
    "ax.set_title('Density Plot of Price Paid Sample (£m)')\n",
    "ax.tick_params(bottom=True, left=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![EDADensityPlot](../images/EDADensityPlot.PNG)\n",
    "\n",
    "This distribution is heavily right skewed due to some houses selling for 100s of millions. Looking at the summary statistics for the dataset indicates that the vast majority of price_paid values are less than £1m, so filtering out values greater than this should give a better view of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![summarystats](../images/summarystats.PNG)\n",
    "\n",
    "Applying this <= £1m filter and also splitting the data by region to produce a ridge plot so that we compare the distributions by region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### postcode area to region lookup\n",
    "outcodeRegion = pd.read_csv(appDir / \"postcode_region_lookup.csv\")\n",
    "outcodeRegion['Area'] = outcodeRegion['Postcode prefix']\n",
    "dataset['Area'] = dataset['Outcode'].replace('\\d+', '', regex=True)\n",
    "df = pd.merge(dataset, outcodeRegion, on=\"Area\", how=\"left\")\n",
    "\n",
    "# filtering data for house prices <= £1m\n",
    "df_filtered = df[df['price_paid_mil'] <= 1]\n",
    "\n",
    "# ridge plot of log price paid by region\n",
    "def ridgePlot(df, xVar, groups):\n",
    "    sns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "    palette = sns.color_palette(\"Set2\", 12)\n",
    "    g = sns.FacetGrid(df, palette=palette, row=groups, hue=groups, aspect=9, height=1.2)\n",
    "    g.map_dataframe(sns.kdeplot, x=xVar, fill=True, alpha=1)\n",
    "    g.map_dataframe(sns.kdeplot, x=xVar, color='black')\n",
    "\n",
    "    def label(x, color, label):\n",
    "        ax = plt.gca()\n",
    "        ax.text(0, .2, label, color='black', fontsize=13,\n",
    "                ha=\"left\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "    g.map(label, groups)\n",
    "    g.figure.subplots_adjust(hspace=-.5)\n",
    "    g.set_titles(\"\")\n",
    "    g.set(yticks=[], xlabel=\"Price Paid £m\", ylabel=\"\")\n",
    "    g.despine(left=True)\n",
    "\n",
    "# ridgePlot(df, \"log_price_paid_mil\", \"UK region\")\n",
    "ridgePlot(df_filtered, \"price_paid_mil\", \"UK region\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![EDARidgePlot](../images/EDARidgePlot.PNG)\n",
    "\n",
    "This shows that all regions have the same type of distribution - though some regions, such as London and South East, are more heavily right skewed. Next I am trying to see if I can fit this distribution to a known distribution, specifically the lognormal distribution (a distribution is lognormal if its log follow a normal distribution):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "### determining the distribution type\n",
    "shape, location, scale = scipy.stats.lognorm.fit(df_filtered['price_paid_mil'])\n",
    "mu, sigma = np.log(scale), shape\n",
    "xmin, xmax = np.min(df_filtered['price_paid_mil']), np.max(df_filtered['price_paid_mil'])\n",
    "x = np.linspace(xmin, xmax, 1000)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(x, scipy.stats.lognorm.pdf(x, shape, location, scale), label=\"Lognormal\")\n",
    "sns.kdeplot(df_filtered, x=\"price_paid_mil\", label=\"Density Plot\")\n",
    "ax.set_xlabel('Price Paid (£m)')\n",
    "ax.set_ylim(bottom=-0.1)\n",
    "ax.set_title('Price Paid Sample (£m) - Density Plot vs Fitted Lognormal Distribution')\n",
    "ax.tick_params(bottom=True, left=True)\n",
    "plt.legend(facecolor='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![EDAFitToLognormal](../images/EDAFitToLognormal.PNG)\n",
    "\n",
    "Except for some of the data points at the peak of the curve, this appears to give a very close fit. We could conclude from this that the overall distribution of house prices less than £1m closely fits a lognormal distribution. We can check this with a QQ plot of the log of house price against a reference normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "df_filtered['log_price_paid_mil'] = np.log(df_filtered['price_paid_mil'])\n",
    "\n",
    "### QQ plot\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "sm.qqplot(data=df_filtered['log_price_paid_mil'], line='s', ax=ax[0])\n",
    "sns.kdeplot(df_filtered['log_price_paid_mil'], ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![EDAQQPlot](../images/EDAQQPlot.PNG)\n",
    "\n",
    "The log of price paid closely matches the normal distribution in the centre, but deviates significantly in the tails, suggesting that the data does not actually quite follow a lognormal distribution. Instead of using the lognormal parameters as summary statistics in my visualisation to describe the data, I will use non-parametric statistics such as the median and IQR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transforming and aggregating the data\n",
    "\n",
    "Before the data can be used in the Shiny app, it needs to be aggregated by Outcode. Doing this outside the Shiny app and reading the aggregated data directly instead greatly improves the performance of the app. I am aggregating by Outcode and YearBin and calculating summary statistics - the summary statistics are chosen taking into consideration the highly right skewed shape of the pseudo-lognormal distribution - the median will be a better estimate of the average value than the arithmetic mean (because it's not affected by outliers), and the IQR will be a better estimate of spread for the same reason. I am also including skew since we know that the distributions of some regions are more heavily skewed than others.\n",
    "\n",
    "~~~~sql\n",
    "CREATE TABLE minMaxMeanCountStddev AS\n",
    "SELECT Outcode, YearBin\n",
    ",MIN(price_paid) AS min\n",
    ",MAX(price_paid) AS max\n",
    ",AVG(price_paid) AS mean\n",
    ",STD(price_paid) AS stddev\n",
    ",COUNT(*) AS count\n",
    "FROM pricePaid\n",
    "GROUP BY Outcode, YearBin;\n",
    "\n",
    "CREATE TABLE skew AS\n",
    "SELECT t1.Outcode, t1.YearBin\n",
    ",SUM(POWER((price_paid - t1.mean), 3) / ((t1.count - 1) * POWER(t1.stddev, 3))) as skew\n",
    "FROM minMaxMeanCountStddev t1\n",
    "INNER JOIN pricePaid pp on t1.Outcode = pp.Outcode and t1.YearBin = pp.YearBin\n",
    "GROUP BY t1.Outcode, t1.YearBin;\n",
    "\n",
    "CREATE TABLE median AS\n",
    "SELECT Outcode, YearBin\n",
    ",AVG(price_paid) AS median\n",
    "FROM (\n",
    "SELECT Outcode, YearBin\n",
    ",price_paid\n",
    ",ROW_NUMBER() OVER (PARTITION BY Outcode, YearBin ORDER BY price_paid) rn\n",
    ",COUNT(*) OVER (PARTITION BY Outcode, YearBin) cnt\n",
    "FROM pricePaid\n",
    ") dd\n",
    "WHERE rn IN (FLOOR((cnt + 1) / 2), FLOOR((cnt + 2) / 2))\n",
    "GROUP BY Outcode, YearBin;\n",
    "\n",
    "CREATE TABLE lowerQuartile AS\n",
    "SELECT Outcode, YearBin\n",
    ",AVG(price_paid) AS lowerQuartile\n",
    "FROM (\n",
    "SELECT Outcode, YearBin\n",
    ",price_paid\n",
    ",ROW_NUMBER() OVER (PARTITION BY Outcode, YearBin ORDER BY price_paid) rn\n",
    ",COUNT(*) OVER (PARTITION BY Outcode, YearBin) cnt\n",
    "FROM pricePaid\n",
    ") dd\n",
    "WHERE rn IN (FLOOR((cnt + 1) / 4), FLOOR((cnt + 2) / 4))\n",
    "GROUP BY Outcode, YearBin;\n",
    "\n",
    "CREATE TABLE upperQuartile AS\n",
    "SELECT Outcode, YearBin\n",
    ",AVG(price_paid) AS upperQuartile\n",
    "FROM (\n",
    "SELECT Outcode, YearBin\n",
    ",price_paid\n",
    ",ROW_NUMBER() OVER (PARTITION BY Outcode, YearBin ORDER BY price_paid) rn\n",
    ",COUNT(*) OVER (PARTITION BY Outcode, YearBin) cnt\n",
    "FROM pricePaid\n",
    ") dd\n",
    "WHERE rn IN (FLOOR(3 * (cnt + 1) / 4), FLOOR(3 * (cnt + 2) / 4))\n",
    "GROUP BY Outcode, YearBin;\n",
    "\n",
    "SELECT t1.*, t2.median, t3.lowerQuartile, t4.upperQuartile, t5.skew\n",
    "FROM minMaxMeanCountStddev t1\n",
    "INNER JOIN median t2 on t1.Outcode = t2.Outcode and t1.YearBin = t2.YearBin\n",
    "INNER JOIN lowerQuartile t3 on t1.Outcode = t3.Outcode and t1.YearBin = t3.YearBin\n",
    "INNER JOIN upperQuartile t4 on t1.Outcode = t4.Outcode and t1.YearBin = t4.YearBin\n",
    "INNER JOIN skew t5 on t1.Outcode = t5.Outcode and t1.YearBin = t5.YearBin;\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the aggregated data into Python and additionally prepare a GeoJSON file needed to be created so that the Shiny app knows where the boundaries of each Outcode area are.\n",
    "\n",
    "**change this **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "### Load data\n",
    "appDir = Path(__file__).parent\n",
    "print(\"Importing data...\")\n",
    "dataset = pd.read_csv(appDir / \"pricepaidsample.csv\", delimiter='\\t', header=0, encoding=\"utf-8\")\n",
    "# remove missing postcodes and price_paid values of zero\n",
    "dataset = dataset[~(dataset['postcode'].isnull() | dataset['price_paid'] == 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A folder containing a GeoJSON file with the polygon coordinates of each Outcode was downloaded online (https://github.com/mhl/postcodes-mapit). This is combined into a single GeoJSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "### Get polygon coordinates (GeoJSON) of each PostcodeArea\n",
    "geojsonDir = appDir / 'districts'\n",
    "# combine all PostcodeArea datasets, one for each PostcodeArea\n",
    "geojsonDict = {}\n",
    "print(\"Importing geojson files...\")\n",
    "for file in geojsonDir.glob('*.geojson'):\n",
    "    with open(file, 'r') as f:\n",
    "        geojsonDict[str(file).split('\\\\')[-1][:-8]] = json.load(f)\n",
    "# add id string to link to summary data\n",
    "for key in geojsonDict.keys():\n",
    "    geojsonDict[key]['features'][0]['id'] = key\n",
    "# changing format of geojsonDict to meet required format for Choropleth function\n",
    "geojsonList = []\n",
    "uniqueOutcodes = dataset['Outcode'].unique().tolist()\n",
    "for outcode in geojsonDict.keys():\n",
    "    features = geojsonDict[outcode]['features']\n",
    "    if outcode in uniqueOutcodes:\n",
    "        geojsonList.append(features[0])\n",
    "geojsonDict = {}\n",
    "geojsonDict['type'] = 'FeatureCollection'\n",
    "geojsonDict['features'] = geojsonList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary statistics (mean, median, min, max) of the price paid were calculated for each Outcode and YearBin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "### aggregate data import by Outcode and YearBin\n",
    "summary = dataset.groupby([\"Outcode\", \"YearBin\"])['price_paid'].agg(['min', 'max', 'mean', 'median']).reset_index()\n",
    "# make sure there is a row for every Outcode and YearBin - set aggregate values to null if missing\n",
    "uniqueYearBins = dataset['YearBin'].unique().tolist()\n",
    "crossJoin = list(itertools.product(uniqueOutcodes, uniqueYearBins))\n",
    "crossJoin = pd.DataFrame(crossJoin, columns=[\"Outcode\", \"YearBin\"])\n",
    "summary = pd.merge(crossJoin, summary, how=\"left\", on=[\"Outcode\", \"YearBin\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the final step in the data preparation, the GeoJSON file and summary dataset were exported to the folder so that they can be read into the Shiny app:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export as geojson dictionary as json file\n",
    "with open(appDir / 'OutcodeCoordinates.json', 'w') as fp:\n",
    "    json.dump(geojsonDict, fp)\n",
    "\n",
    "# export summary dataframe as csv\n",
    "summary.to_csv(appDir / 'summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building the Shiny app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The app.py program of the Shiny app consists of 3 main parts:\n",
    "\n",
    "- importing the data\n",
    "- building the HTML interface\n",
    "- defining a server function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from ipyleaflet import Map, Choropleth\n",
    "from shiny import App, Inputs, Outputs, Session, ui, reactive\n",
    "from shinywidgets import output_widget, render_widget\n",
    "from branca.colormap import linear\n",
    "\n",
    "### Load data\n",
    "appDir = Path(os.path.abspath(''))\n",
    "with open(appDir / 'OutcodeCoordinates.json', 'r') as f:\n",
    "    outcodeCoordinates = json.load(f)\n",
    "summary = pd.read_csv(appDir / 'summary.csv')\n",
    "yearBins = list(summary['YearBin'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Building the HTML interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nest Python functions to build an HTML interface\n",
    "app_ui = ui.page_fillable( \n",
    "    # Layout the UI with Layout Functions\n",
    "    # Add Inputs with ui.input_*() functions \n",
    "    # Add Outputs with ui.output_*() functions\n",
    "    ui.layout_sidebar(\n",
    "        ui.sidebar(\n",
    "            ui.input_checkbox_group('yearBin', \"Time Period\", yearBins),\n",
    "            ui.input_select('statistic', \"House Price Summary Statistic\", ['mean', 'median', 'min', 'max'], selected='mean', multiple=False),\n",
    "            ui.input_switch(\"switch\", \"Compare Summary Statistic between Time Periods\", False)\n",
    "        ),\n",
    "        ui.card(\n",
    "            output_widget(\"map\", fill=True),\n",
    "            full_screen=True,\n",
    "            fill=True\n",
    "        ),\n",
    "        full_screen=True\n",
    "    ),\n",
    "    title=\"House Prices Visualisation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in a HTML template that looks like this:\n",
    "\n",
    "![HTMLTemplate](../images/HTMLTemplate.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Defining a server function\n",
    "\n",
    "Within the server function, the createChoroData function is reactive and will be called every time one of the inputs to the app changes. The functions uses the user inputs the filter the dataset and return a dictionary containing a key for each Outcode and a value corresponding to the summary statistic.\n",
    "\n",
    "The createChoroData function also contains logic for if more than one time period is selected - in this case the function will calculate the difference between the summary statistics of the earliest and latest time period selected.\n",
    "\n",
    "The map function creates an ipyleaflet Map object and adds a Choropleth layer to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define server\n",
    "def server(input: Inputs, output: Outputs, session: Session):\n",
    "    # function to filter the summary dataset and return a lookup dictionary with a key for each Outcode\n",
    "    @reactive.calc\n",
    "    def createChoroData():\n",
    "\n",
    "        # select all time periods if none are selected\n",
    "        if input.yearBin() == tuple():\n",
    "            filter = yearBins\n",
    "        else:\n",
    "            filter = list(input.yearBin())\n",
    "\n",
    "        # logic for comparing summary stastics between time periods\n",
    "        if input.switch():\n",
    "            minYearBin = filter[0]\n",
    "            maxYearBin = filter[-1]\n",
    "            dfMin = summary[summary['YearBin'] == minYearBin][['Outcode', input.statistic()]]\n",
    "            dfMax = summary[summary['YearBin'] == maxYearBin][['Outcode', input.statistic()]]\n",
    "            df = pd.merge(dfMin, dfMax, how=\"inner\", on=\"Outcode\")\n",
    "            df['diff'] = df[input.statistic() + '_y'] - df[input.statistic() + '_x']\n",
    "            df = df.set_index('Outcode')\n",
    "            return df['diff'].to_dict()\n",
    "        # if not comparing time periods then just show summary statistic\n",
    "        else:\n",
    "            df = summary[summary['YearBin'].isin(filter)].set_index('Outcode')\n",
    "            return df[input.statistic()].to_dict()\n",
    "\n",
    "    ### For each output, define a function that generates the output\n",
    "    @render_widget  \n",
    "    def map():\n",
    "        # create a Map object and add a Choropleth layer to it\n",
    "        m = Map(center=(54.00366, -2.547855), zoom=5.5, zoom_snap=0.2)\n",
    "\n",
    "        layer = Choropleth(\n",
    "                    geo_data=outcodeCoordinates,\n",
    "                    choro_data=createChoroData(),\n",
    "                    key_on='id',\n",
    "                    colormap=linear.viridis,\n",
    "                    border_color='black',\n",
    "                    style={'fillOpacity': 0.8, 'dashArray': '5, 5'}\n",
    "                )\n",
    "        \n",
    "        m.add(layer)\n",
    "\n",
    "        return m\n",
    "\n",
    "# Call App() to combine app_ui and server() into an interactive app\n",
    "app = App(app_ui, server)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transforming the data (again)\n",
    "\n",
    "The Shiny app is now working, however it is not very insightful because (except for a couple of areas in London) every area appears to be the same colour:\n",
    "\n",
    "<img src=\"../images/MapNoTransformation.PNG\" alt=\"MapNoTransformation\" width=\"20px\" class=\"bg-primary\">\n",
    "\n",
    "The density plot below, of all of the house prices in the sample, shows what the problem is. The data is highly right skewed, and because the colour map works by assigning a different colour to each quantile, the majority of data has the same colour.\n",
    "\n",
    "**need to change this to plot the distribution of the means instead of price_paid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "### Load data\n",
    "appDir = Path(__file__).parent\n",
    "dataset = pd.read_csv(appDir / \"pricepaidsample.csv\", delimiter='\\t', header=0, encoding=\"utf-8\")\n",
    "# remove missing postcodes and price_paid values of zero\n",
    "dataset = dataset[~(dataset['postcode'].isnull() | dataset['price_paid'] == 0)]\n",
    "\n",
    "dataset['price_paid_mil'] = dataset['price_paid'] / 1000000\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 12))\n",
    "sm.qqplot(dataset['price_paid_mil'], ax=ax[1])\n",
    "sns.kdeplot(data=dataset, x='price_paid_mil', ax=ax[0])\n",
    "ax[0].set_xlabel('Price Paid (£m)')\n",
    "ax[0].set_xlim(left=0.0)\n",
    "ax[1].set_ylim(bottom=0.0)\n",
    "ax[0].set_title('Density Plot of Price Paid Sample (£m)')\n",
    "ax[1].set_title('QQ Plot of Price Paid Sample (£m)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DensityPlot](../images/DensityPlot.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To resolve this, the QuantileTransformer function from scikit-learn was used to mathematically transform the data to follow a more normal distribution. This is a non-parametric method that works by mapping each quantile of the input distribution onto those of the output distribution (in this case the normal distribution) and applying the required transformation to each quantile so that they match as closely as possible. This is often used to transform features for machine learning problems because certain machine learning algorithms cannot effectively deal with non-normal features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import numpy as np\n",
    "\n",
    "qt = QuantileTransformer(output_distribution='normal', random_state=0)\n",
    "values = qt.fit_transform(np.array(list(dataset['price_paid_mil'])).reshape(-1, 1)).flatten()\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10,12))\n",
    "sm.qqplot(data=np.array(values), ax=ax[1])\n",
    "sns.kdeplot(data=values, ax=ax[0])\n",
    "ax[0].set_xlabel('Price Paid (£m)')\n",
    "ax[0].set_title('Density Plot of Price Paid Sample with Quantile Transformation (£m)')\n",
    "ax[1].set_title('QQ Plot of Price Paid Sample with Quantile Transformation (£m)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DensityPlotQuantileTransformed](../images/DensityPlotQuantileTransformed.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in a distribution that is much more normal, so should make more sense when the colour map is applied to it. However, we can see from the QQ plot that the distribution deviates from normality at each extreme.\n",
    "\n",
    "The following function was added to the Shiny app and applied to the lookup dictionary returned by the createChoroData function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantileTransformer(dictionary):\n",
    "    qt = QuantileTransformer(output_distribution='normal', random_state=0)\n",
    "    values = qt.fit_transform(np.array(list(dictionary.values())).reshape(-1, 1)).flatten()\n",
    "    return dict(zip(dictionary.keys(), values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This change to the app produces the map below:\n",
    "\n",
    "![MapQuantileTransformed](../images/MapQuantileTransformed.PNG)\n",
    "\n",
    "This is definitely an improvement, as we can now see some colour variation across the map, however the range of colours is still quite narrow. This is due to the long tails shown in the density plot.\n",
    "\n",
    "Another approach to this problem was to bin the data into deciles, adding the following step to the createChoroData function before returning a dictionary with the decile as the value in each key:value pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['decile'] = pd.qcut(df[summary_statistic], 10, labels=False) # summary_statistic is either input.statistic() or 'diff'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the deciles of the summary statistic gives a much better colour variation across the map, clearly highlighting the regional differences. This method has the added benefit of being conceptually simple, with the top 10% of values being assigned to the brightest colour and bottom 10% being assigned to the darkest colour.\n",
    "\n",
    "![MapDeciles](../images/MapDeciles.PNG)\n",
    "\n",
    "##### GeoJSON compression\n",
    "\n",
    "The app is now working and displaying a useful colour map, however it is extremely slow to respond to any user input. This is because of the large size of the GeoJSON file, which has to be processed every time a change is made to the inputs. An easy work-around to this problem was to use https://mapshaper.org/ to simplify the GeoJSON file, reducing its size using the Visvalingam / weighted area method with a 1% zoom level. The import was then changed to read from this GeoJSON file instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(appDir / 'OutcodeCoordinates_compressed.json', 'r') as f:\n",
    "    outcodeCoordinates = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis\n",
    "\n",
    "What summary statistics are best to describe the data? If I know which distribution the data fits most closely then that would be helpful.\n",
    "\n",
    "Do different areas follow different types of distribution?\n",
    "\n",
    "Has the type of distribution changed over time?\n",
    "\n",
    "**Min, Max**\n",
    "\n",
    "Looking at all time periods, the min shows a random pattern with no clear trend by area.\n",
    "\n",
    "**Mean and Median**\n",
    "\n",
    "**Which areas have had the greatest change in median over time?**\n",
    "\n",
    "**Which areas have had the greatest change in max over time?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
