{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices\n",
    "\n",
    "This project involved building an interactive web app, using Shiny for Python, to display house prices in England and Wales, adding a Choropleth layer to visualise the differences by region. The aim was to demonstrate the Shiny software and investigate trends in house prices over time and across different regions of England and Wales.\n",
    "\n",
    "##### About the data\n",
    "\n",
    "Write description here of how I got the data and how it's created/published.\n",
    "\n",
    "##### Initial planning\n",
    "\n",
    "- What granularity to use? e.g. county, town, postcode\n",
    "    - The dataset contains the postcode of each property, and every postcode has a clearly defined area, which should be available online\n",
    "    - The whole postcode is likely too granular - using the area code or district code should be a good compromise\n",
    "- How to aggregate price paid by area?\n",
    "    - Min, Max, Median, Mean\n",
    "    - Allow user to choose which statistic to show on the map\n",
    "    - Could also compare these summary statistics between points in time, which would highlight the areas which have seen the greatest changes in price over time\n",
    "\n",
    "##### Preparing the data\n",
    "\n",
    "Loading the CSV file into a MySQL database.\n",
    "\n",
    "~~~~sql\n",
    "DROP DATABASE IF EXISTS `houseprices`;\n",
    "CREATE DATABASE `houseprices`;\n",
    "USE `houseprices`;\n",
    "\n",
    "CREATE TABLE `pricepaid` (\n",
    "`unique_id` VARCHAR(100),\n",
    "`price_paid` DECIMAL,\n",
    "`deed_date` DATE,\n",
    "`postcode` VARCHAR(8),\n",
    "`property_type` VARCHAR(1),\n",
    "`new_build` VARCHAR(1),\n",
    "`estate_type` VARCHAR(1),\n",
    "`saon` VARCHAR(50),\n",
    "`paon` VARCHAR(50),\n",
    "`street` VARCHAR(50),\n",
    "`locality` VARCHAR(50),\n",
    "`town` VARCHAR(50),\n",
    "`district` VARCHAR(50),\n",
    "`county` VARCHAR(50),\n",
    "`transaction_category` VARCHAR(1),\n",
    "`linked_data_uri` VARCHAR(1),\n",
    "PRIMARY KEY (unique_id)\n",
    ");\n",
    "\n",
    "SET GLOBAL local_infile=ON;\n",
    "SET autocommit=0;\n",
    "SET unique_checks=1;\n",
    "SET foreign_key_checks=0;\n",
    "\n",
    "LOAD DATA LOW_PRIORITY \n",
    "LOCAL INFILE 'Path/To/Project/pricepaid.csv'\n",
    "INTO TABLE pricepaid \n",
    "CHARACTER SET armscii8\n",
    "FIELDS TERMINATED BY ','\n",
    "ENCLOSED BY '\"'\n",
    "LINES TERMINATED BY '\\n' \n",
    "(`unique_id`,`price_paid`,`deed_date`,`postcode`,`property_type`,`new_build`,`estate_type`,`saon`,`paon`,`street`,`locality`,`town`,`district`,`county`,`transaction_category`,`linked_data_uri`);\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had originally planned to use the full dataset in my Shiny app, however the full table is ~5GB in size with ~29m rows. I chose to get around this by sampling the dataset. Taking a simple random sample of the data would mean that the number of samples from each area would be proportional to the population of that area, so to ensure that each area had an equal number of samples I would use stratified sampling instead.\n",
    "\n",
    "I needed to choose a level of granularity to which to stratify the data. A UK postcode is made up of 2 parts, the outward code (first part) and inward code (second part), separated by a space. The outward code consists of the postcode area (either 1 or 2 letters) followed by the postcode district (usually 1 or 2 digits). For example, in the postcode PO16 7GZ, PO16 is the outward code (or outcode), PO is the area and 16 is the district.\n",
    "\n",
    "OutCode and PostcodeArea were added as generated columns to the pricepaid table, along with a Year column and a YearBin column.\n",
    "\n",
    "~~~~sql\n",
    "ALTER TABLE pricepaid ADD COLUMN OutCode VARCHAR(4) GENERATED ALWAYS AS substr(postcode, 1, locate(' ', postcode) - 1) STORED;\n",
    "ALTER TABLE pricepaid ADD COLUMN PostcodeArea VARCHAR(3) GENERATED ALWAYS AS regexp_replace(OutCode, '[0-9]+', '') STORED;\n",
    "ALTER TABLE pricepaid ADD COLUMN Year INT GENERATED ALWAYS AS year(cast(deed_date as date)) STORED;\n",
    "ALTER TABLE pricepaid ADD COLUMN YearBin VARCHAR(4) GENERATED ALWAYS AS case when (Year < 2005) then '1995 - 2004' when (Year < 2015) then '2005 - 2014' else '2015 +' end STORED;\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an index on Outcode and YearBin to speed up the stratified sample query.\n",
    "\n",
    "~~~~sql\n",
    "CREATE INDEX OutcodeYearBinIndex ON pricepaid (Outcode, YearBin);\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a stratified sample of 100 observations for each distinct OutCode and YearBin.\n",
    "\n",
    "~~~~sql\n",
    "SELECT t.* FROM\n",
    "(SELECT pp.*, ROW_NUMBER() OVER (PARTITION BY OutCode, YearBin ORDER BY RAND()) AS SeqNum\n",
    "FROM pricepaid pp) t\n",
    "WHERE t.SeqNum <= 100\n",
    "INTO LOCAL OUTFILE '/Path/To/Project/pricepaidsample.csv'\n",
    "FIELDS TERMINATED BY ','\n",
    "ENCLOSED BY '\"'\n",
    "LINES TERMINATED BY '\\n';\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transforming and aggregating the data\n",
    "\n",
    "Before the data can be used in the Shiny app, it needs to be aggregated by area. Doing this outside the Shiny app and instead reading the aggregated data directly improves the performance of the app.\n",
    "\n",
    "Additionally, a GeoJSON file needed to be created so that the Shiny app knows where the boundaries of each postcode area are.\n",
    "\n",
    "First, the sample CSV is read into a Python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "### Load data\n",
    "appDir = Path(__file__).parent\n",
    "print(\"Importing data...\")\n",
    "dataset = pd.read_csv(appDir / \"pricepaidsample.csv\", delimiter='\\t', header=0, encoding=\"utf-8\")\n",
    "# remove missing postcodes\n",
    "dataset = dataset[~dataset['postcode'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A folder containing a GeoJSON file with the polygon coordinates of each Outcode was downloaded online. This is combined into a single GeoJSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "### Get polygon coordinates (GeoJSON) of each PostcodeArea\n",
    "geojsonDir = appDir / 'districts'\n",
    "# combine all PostcodeArea datasets, one for each PostcodeArea\n",
    "geojsonDict = {}\n",
    "print(\"Importing geojson files...\")\n",
    "for file in geojsonDir.glob('*.geojson'):\n",
    "    with open(file, 'r') as f:\n",
    "        geojsonDict[str(file).split('\\\\')[-1][:-8]] = json.load(f)\n",
    "# add id string to link to summary data\n",
    "for key in geojsonDict.keys():\n",
    "    geojsonDict[key]['features'][0]['id'] = key\n",
    "# changing format of geojsonDict to meet required format for Choropleth function\n",
    "geojsonList = []\n",
    "uniqueOutcodes = dataset['Outcode'].unique().tolist()\n",
    "for outcode in geojsonDict.keys():\n",
    "    features = geojsonDict[outcode]['features']\n",
    "    if outcode in uniqueOutcodes:\n",
    "        geojsonList.append(features[0])\n",
    "geojsonDict = {}\n",
    "geojsonDict['type'] = 'FeatureCollection'\n",
    "geojsonDict['features'] = geojsonList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary statistics (mean, median, min, max) of the price paid were calculated for each Outcode and YearBin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "### aggregate data import by Outcode and YearBin\n",
    "summary = dataset.groupby([\"Outcode\", \"YearBin\"])['price_paid'].agg(['min', 'max', 'mean', 'median']).reset_index()\n",
    "# make sure there is a row for every Outcode and YearBin - set aggregate values to null if missing\n",
    "uniqueYearBins = dataset['YearBin'].unique().tolist()\n",
    "crossJoin = list(itertools.product(uniqueOutcodes, uniqueYearBins))\n",
    "crossJoin = pd.DataFrame(crossJoin, columns=[\"Outcode\", \"YearBin\"])\n",
    "summary = pd.merge(crossJoin, summary, how=\"left\", on=[\"Outcode\", \"YearBin\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the final step in the data preparation, the GeoJSON file and summary dataset were exported to the folder so that they can be read into the Shiny app:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export as geojson dictionary as json file\n",
    "with open(appDir / 'OutcodeCoordinates.json', 'w') as fp:\n",
    "    json.dump(geojsonDict, fp)\n",
    "\n",
    "# export summary dataframe as csv\n",
    "summary.to_csv(appDir / 'summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building the Shiny app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The app.py program of the Shiny app consists of 3 main parts:\n",
    "\n",
    "- importing the data\n",
    "- building the HTML interface\n",
    "- defining a server function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from ipyleaflet import Map, Choropleth\n",
    "from shiny import App, Inputs, Outputs, Session, ui, reactive\n",
    "from shinywidgets import output_widget, render_widget\n",
    "from branca.colormap import linear\n",
    "\n",
    "### Load data\n",
    "appDir = Path(os.path.abspath(''))\n",
    "with open(appDir / 'OutcodeCoordinates.json', 'r') as f:\n",
    "    outcodeCoordinates = json.load(f)\n",
    "summary = pd.read_csv(appDir / 'summary.csv')\n",
    "yearBins = list(summary['YearBin'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Building the HTML interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nest Python functions to build an HTML interface\n",
    "app_ui = ui.page_fillable( \n",
    "    # Layout the UI with Layout Functions\n",
    "    # Add Inputs with ui.input_*() functions \n",
    "    # Add Outputs with ui.output_*() functions\n",
    "    ui.layout_sidebar(\n",
    "        ui.sidebar(\n",
    "            ui.input_checkbox_group('yearBin', \"Time Period\", yearBins),\n",
    "            ui.input_select('statistic', \"House Price Summary Statistic\", ['mean', 'median', 'min', 'max'], selected='mean', multiple=False),\n",
    "            ui.input_switch(\"switch\", \"Compare Summary Statistic between Time Periods\", False)\n",
    "        ),\n",
    "        ui.card(\n",
    "            output_widget(\"map\", width=\"auto\", height=\"auto\")\n",
    "        )\n",
    "    ),\n",
    "    title=\"UK House Prices Visualisation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in a HTML template that looks like this:\n",
    "\n",
    "![HTMLTemplate](../images/HTMLTemplate.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Defining a server function\n",
    "\n",
    "Within the server function, the createChoroData function is reactive and will be called every time one of the inputs to the app changes. The functions uses the user inputs the filter the dataset and return a dictionary containing a key for each Outcode and a value corresponding to the summary statistic.\n",
    "\n",
    "The createChoroData function also contains logic for if more than one time period is selected - in this case the function will calculate the difference between the summary statistics of the earliest and latest time period selected.\n",
    "\n",
    "The map function creates an ipyleaflet Map object and adds a Choropleth layer to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define server\n",
    "def server(input: Inputs, output: Outputs, session: Session):\n",
    "    # function to filter the summary dataset and return a lookup dictionary with a key for each Outcode\n",
    "    @reactive.calc\n",
    "    def createChoroData():\n",
    "\n",
    "        # select all time periods if none are selected\n",
    "        if input.yearBin() == tuple():\n",
    "            filter = yearBins\n",
    "        else:\n",
    "            filter = list(input.yearBin())\n",
    "\n",
    "        # logic for comparing summary stastics between time periods\n",
    "        if input.switch():\n",
    "            minYearBin = filter[0]\n",
    "            maxYearBin = filter[-1]\n",
    "            dfMin = summary[summary['YearBin'] == minYearBin][['Outcode', input.statistic()]]\n",
    "            dfMax = summary[summary['YearBin'] == maxYearBin][['Outcode', input.statistic()]]\n",
    "            df = pd.merge(dfMin, dfMax, how=\"inner\", on=\"Outcode\")\n",
    "            df['diff'] = df[input.statistic() + '_y'] - df[input.statistic() + '_x']\n",
    "            df = df.set_index('Outcode')\n",
    "            return df['diff'].to_dict()\n",
    "        # if not comparing time periods then just show summary statistic\n",
    "        else:\n",
    "            df = summary[summary['YearBin'].isin(filter)].set_index('Outcode')\n",
    "            return df[input.statistic()].to_dict()\n",
    "\n",
    "    ### For each output, define a function that generates the output\n",
    "    @render_widget  \n",
    "    def map():\n",
    "        # create a Map object and add a Choropleth layer to it\n",
    "        m = Map(center=(54.00366, -2.547855), zoom=5.5, zoom_snap=0.2)\n",
    "\n",
    "        layer = Choropleth(\n",
    "                    geo_data=outcodeCoordinates,\n",
    "                    choro_data=createChoroData(),\n",
    "                    key_on='id',\n",
    "                    colormap=linear.viridis,\n",
    "                    border_color='black',\n",
    "                    style={'fillOpacity': 0.8, 'dashArray': '5, 5'}\n",
    "                )\n",
    "        \n",
    "        m.add(layer)\n",
    "\n",
    "        return m\n",
    "\n",
    "# Call App() to combine app_ui and server() into an interactive app\n",
    "app = App(app_ui, server)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transforming the data (again)\n",
    "\n",
    "The Shiny app is now working, however it is not very insightful because (except for a couple of areas in London) every area appears to be the same colour:\n",
    "\n",
    "<img src=\"../images/ColourMap.PNG\" alt=\"ColourMap\" scale=0.2>\n",
    "<img src=\"../images/MapNoTransformation.PNG\" alt=\"MapNoTransformation\" width=\"20px\" class=\"bg-primary\">\n",
    "\n",
    "The density plot below, of all of the house prices in the sample, shows what the problem is. The data is highly right skewed, and because the colour map works by assigning a different colour to each quantile, the majority of data has the same colour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "### Load data\n",
    "appDir = Path(__file__).parent\n",
    "dataset = pd.read_csv(appDir / \"pricepaidsample.csv\", delimiter='\\t', header=0, encoding=\"utf-8\")\n",
    "# remove missing postcodes\n",
    "dataset = dataset[~dataset['postcode'].isnull()]\n",
    "\n",
    "dataset['price_paid_mil'] = dataset['price_paid'] / 1000000\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 12))\n",
    "sm.qqplot(dataset['price_paid_mil'], ax=ax[1])\n",
    "sns.kdeplot(data=dataset, x='price_paid_mil', ax=ax[0])\n",
    "ax[0].set_xlabel('Price Paid (£m)')\n",
    "ax[0].set_xlim(left=0.0)\n",
    "ax[1].set_ylim(bottom=0.0)\n",
    "ax[0].set_title('Density Plot of Price Paid Sample (£m)')\n",
    "ax[1].set_title('QQ Plot of Price Paid Sample (£m)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DensityPlot](../images/DensityPlot.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To resolve this, the QuantileTransformer function from scikit-learn was used to mathematically transform the data to follow a more normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import numpy as np\n",
    "\n",
    "qt = QuantileTransformer(output_distribution='normal', random_state=0)\n",
    "values = qt.fit_transform(np.array(list(dataset['price_paid_mil'])).reshape(-1, 1)).flatten()\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10,12))\n",
    "sm.qqplot(data=np.array(values), ax=ax[1])\n",
    "sns.kdeplot(data=values, ax=ax[0])\n",
    "ax[0].set_xlabel('Price Paid (£m)')\n",
    "ax[0].set_title('Density Plot of Price Paid Sample with Quantile Transformation (£m)')\n",
    "ax[1].set_title('QQ Plot of Price Paid Sample with Quantile Transformation (£m)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DensityPlot](../images/DensityPlotQuantileTransformed.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in a distribution that is much more normal, so should make more sense when the colour map is applied to it. However, we can see from the QQ plot that the distribution deviates from normality at each extreme.\n",
    "\n",
    "The following function was added to the Shiny app and applied to the lookup dictionary returned by the createChoroData function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantileTransformer(dictionary):\n",
    "    qt = QuantileTransformer(output_distribution='normal', random_state=0)\n",
    "    values = qt.fit_transform(np.array(list(dictionary.values())).reshape(-1, 1)).flatten()\n",
    "    return dict(zip(dictionary.keys(), values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This change to the app produces the map below:\n",
    "\n",
    "![NoTransformation](../images/MapQuantileTransformed.PNG)\n",
    "\n",
    "This is definitely an improvement, as we can now see some colour variation across the map, however the range of colours is still quite narrow. This is due to the long tails shown in the density plot.\n",
    "\n",
    "Another approach to this problem was to bin the data into deciles, adding the following step to the createChoroData function before returning a dictionary with the decile as the value in each key:value pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['decile'] = pd.qcut(df[summary_statistic], 10, labels=False) # summary_statistic is either input.statistic() or 'diff'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the deciles of the summary statistic gives a much better colour variation across the map, clearly highlighting the regional differences. This method has the added benefit of being conceptually simple, with the top 10% of values being assigned to the brightest colour and bottom 10% being assigned to the darkest colour.\n",
    "\n",
    "![MapDeciles](../images/MapDeciles.PNG)\n",
    "\n",
    "##### GeoJSON compression\n",
    "\n",
    "The app is now working and displaying a useful colour map, however it is extremely slow to respond to any user input. This is because of the large size of the GeoJSON file, which has to be processed every time a change is made to the inputs. An easy work-around to this problem was to use https://mapshaper.org/ to simplify the GeoJSON file, reducing its size using the Visvalingam / weighted area method with a 1% zoom level. The import was then changed to read from this GeoJSON file instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(appDir / 'OutcodeCoordinates_compressed.json', 'r') as f:\n",
    "    outcodeCoordinates = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
